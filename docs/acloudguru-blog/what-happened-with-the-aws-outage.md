# AWS 停机发生了什么？

> 原文：<https://acloudguru.com/blog/engineering/what-happened-with-the-aws-outage>

本周 AWS 的头条新闻是 2021 年 12 月 7 日 AWS 停电。发生了什么，为什么？

我们将在本帖中进行讨论，首先从美国 AWS 用户的角度对 AWS 停机事件进行高度描述。然后，我们将浏览 AWS 分享的一些幕后发生的事情。最后，我们将给你留下一些(希望)有价值的收获。

* * *

## 加速您的职业发展

[从 ACG 开始](https://acloudguru.com/pricing)通过 AWS、Microsoft Azure、Google Cloud 等领域的课程和实际动手实验室改变你的职业生涯。

* * *

### 目录

## AWS 停机发生了什么？

2021 年 12 月 7 日上午，美国东部时间上午 10:30/太平洋时间上午 7:30，亚马逊的“us-east-1”地区出了问题:北弗吉尼亚。

在接下来的三分钟内，从我们的外部角度来看，这几乎是突然的，该地区的许多 AWS 服务开始出现问题，包括但不限于:

*   管理控制台
*   53 号公路
*   API 网关
*   EventBridge(以前称为 CloudWatch 事件)
*   亚马逊连接
*   EC2

现在，澄清一下，问题是*而不是*所有这些服务的完全中断。例如，如果问题开始时已经有一个 EC2 实例在运行，那么*很可能会在整个事件中保持*运行良好。

然而，那个正在运行的实例所能做的事情很可能已经受到了影响。例如，EC2 实例在通过不再工作的*VPC 端点连接到仍然工作的*S3 和 DynamoDB 时会遇到问题。**

**此外，该问题不仅影响了 us-east-1 中的所有可用区域，还中断了许多恰好位于该区域的全球服务。这包括 AWS 帐户根登录、单点登录(SSO)和安全令牌服务(STS)。**

**总体影响是广泛的，这个问题给网飞、迪士尼加、Roomba、Ticketmaster 和华尔街日报等服务带来了不同程度的问题。它还影响了许多亚马逊服务，包括 Prime Music、门铃、履行中心的物流应用程序，以及 Amazon.com 购物网站的一些部分，这些应用程序将显示可爱的狗的图片。**

**这是一件大事。所以，当然，人们开始在互联网上讨论。**

**一位名叫“ZeldaFanBoi1988”的 Reddit 用户[写道](https://www.reddit.com/r/aws/comments/rb1xrd/500502_errors_on_aws_console/hnmnz1y/):**

**“既然我什么工作也做不了，我决定放松一下，叫些比萨饼。然后我试着从喷气机的比萨饼网站在线订购。500 个错误。lol。查看网络请求头。这是 AWS…。”**

**但似乎没有人对受停电影响的公司感到太沮丧。事实上，举个例子，几个人反而借此机会分享他们有多爱*Jet 的！***

**编者按:事实上，我可以确定杰特的味道非常好。)**

**无论如何，ZeldaFanBoi1988 确实得到了他们的比萨饼，并报告说:“我像一个农民一样在电话上订购。AWS 真是毁了我的一天。”**

### **状态仪表板和支持票证**

**但是也有一些其他的令人讨厌的问题——甚至比 ZeldaFanBoi1988 还要糟糕，不得不像 2008 年一样通过电话订购比萨饼。**

**首先，尽管存在这些问题，AWS 状态面板持续了太长时间，无法显示所有服务的所有绿灯。**

**第二，不再可能向亚马逊登记支持票，因为他们的支持联络中心也坏了！这种客户沟通让很多人相当不安。**

**将近一个小时后，状态面板才开始报告任何问题，直到底层问题得到解决，服务恢复正常。**

**现在，说到“潜在的问题”，让我们倒回到开始，看看那些问题。**

* * *

### **了解如何像 SRE 人一样思考**

**观看[这一免费点播的网络研讨会](https://get.acloudguru.com/think-like-an-sre-webinar)，了解 Nobl9 工厂可靠性工程总监 Alex Hidalgo 对 SRE 文化和工具的剖析。**

* * *

## **AWS 停机原因:内部问题**

### **内部网络拥塞**

**2021 年 12 月 7 日，东部时间上午 10:30/太平洋时间上午 7:30，亚马逊“us-east-1”地区(北弗吉尼亚州)的一个自动化系统试图扩大运行在 AWS 的*私有*内部网络上的内部服务——他们使用这个网络来*控制和监控*他们所有的亚马逊网络服务。**

**正如 AWS 所描述的，这“触发了内部网络中大量客户端的意外行为”。**

**基本上，AWS 无意中在他们自己的内部网络上触发了分布式拒绝服务(或 DDoS 攻击)。呀。**

**打个比方，就好像生活在某个特定城市的每个人都在同一时间开车去市中心。瞬间拥堵。没有动静。不是救护车。不是新闻记者。甚至连试图解决问题的交警也没有。**

**现在，我们确实知道应该如何避免这样的网络拥塞问题:[我们使用指数补偿和抖动。不幸的是，这需要每个客户做正确的事情，正如 AWS 在他们的报告中写道:“一个潜在的问题阻止了这些客户在这次事件中充分后退。”](https://aws.amazon.com/blogs/architecture/exponential-backoff-and-jitter/)**

**是啊…哎呀！**

### **DNS？**

**所以，AWS 的人有点盲目，因为他们的内部监控被洪水带走了。他们查看日志，认为可能是 DNS。总是 DNS 吧？(甚至还有关于它的俳句。)**

**嗯，在问题出现两个小时后，他们设法完全恢复了内部 DNS 解析。尽管据说这确实有所帮助，但并不能解决所有问题。所以，非常令人惊讶的是，这次是*而不是* DNS。**

## **AWS 停机(完全)解决方案**

**在接下来的三个小时里，AWS 的工程师们疯狂地工作着，尝试着一切。或者，正如 AWS 所说，“运营商继续致力于一系列补救措施，以减少内部网络的拥塞，包括确定流量的主要来源，以隔离到专用网络设备，禁用一些繁重的网络流量服务，并使额外的网络容量在线。”**

**然后，在太平洋时间下午 12:35，或东部时间下午 3:35，AWS 运营商禁用了 EventBridge (CloudWatch 事件)的事件交付，以减少受影响的网络设备上的负载。不管这是关键还是沧海一粟，事情终于开始好转。AWS 报告称，到 1:15，内部网络拥塞“正在改善”，到 1:34，“显著改善”，到太平洋标准时间下午 2:22，“所有网络设备完全恢复”**

**虽然这解决了网络洪水和他们的支持联络中心，但仍然需要更多的时间让所有亚马逊网络服务恢复在线。API Gateway、Fargate 和 EventBridge 是完全稳定最慢的，至少需要等到太平洋时间下午 6:40 或东部时间晚上 9:40。多好的一天啊。**

**您可以在此阅读 AWS 断电事件摘要[。](https://aws.amazon.com/message/12721/)**

## **自动气象站大修的教训**

### **AWS 能从停电中学到什么？**

**好吧。所以 AWS *已经*喊出了他们从这次事件中学到的一些东西。**

**一件关键的事情是，他们需要在运营问题期间更好地与客户沟通，而不是让这些系统同时停机。他们计划在这里进行一些重大升级，但我们必须等待，看看一切进展如何。当然，他们也在努力修复回退错误，加上一些额外的网络级缓解措施，以防止另一场风暴。他们在报告中总结道，“我们将尽一切努力从这次事件中吸取教训，并利用这次事件进一步提高我们的可用性。”**

### **我们能从中断中学到什么？**

**但是我们呢？*我们*能学到什么？**

**在这次活动中，有很多回应——从“把婴儿和洗澡水一起扔出去”到“没有云给你！”到天真的“多云解决一切！”**

**当然，更多的是更温和的“多地区，至少。”但是不要反应过度，因为根据定义，下意识的架构改变是欠考虑的。**

**想想 SRE 的书， *[工地可靠性工程](https://sre.google/sre-book/table-of-contents/)* 。这都是关于如何保持重要系统的运行。我想和你们分享第三章“拥抱风险”中的一段话:**

**".。。然而，超过某一点，增加可靠性对服务(及其用户)来说是更糟而不是更好！极高的可靠性是有代价的。。。可靠性的一次增量提升，成本可能是之前增量的 100 倍。”**

**换个角度来看，以前你一个人一个月可能完成的事情，现在可能需要一个 10 人的团队一整年才能完成。这听起来像是一个好的交易吗？想象一下运行和维护一个如此复杂的系统的持续成本！呀。**

**这本书继续讨论了其中的一些成本，但这一切都归结为需要做出权衡。我认为这句话总结了最重要的一点:**

**".。。站点可靠性工程不是简单地最大化正常运行时间，而是寻求不可用性风险与快速创新和高效服务运营目标之间的平衡，从而优化用户的总体满意度(包括功能、服务和性能)。"**

### **每件事都会失败**

**我们已经了解到，敏捷性是 IT 领域的核心。找出什么对你的用户来说最重要。**

**正如亚马逊首席技术官沃纳·威格尔所说:“任何事情都会失败，一直如此。”**

**现在，我们完全有可能想出策略和架构来避免我们被这个特殊问题的重复所影响。但如果这是一件简单的事情，无论是通过多地区还是其他方式，亚马逊早就为 AWS 状态面板这样的东西做了。**

**但是正如 AWS 所指出的，“网络拥塞损害了我们的服务健康仪表板工具，使其无法适当地故障转移到我们的备用区域。”**

**没错。他们实际上*确实*有多区域设置，但是他们的故障转移机制失败了。比如丢了你末日地堡的钥匙然后被锁在外面。理论上有准备，实际上没有。他们是非常聪明的工程师，但他们也是人。**

**实际上，事情会变得复杂——尤其是因为你不知道事情会如何失败。**

**当我们过去不得不自己在简单的实例上构建和管理一切时，失败更容易预测:实例会死亡或变得不可用。但是，当我们利用托管服务时，我们最终会遇到各种不同的故障。**

**现在，要明确的是，仅仅因为托管服务有时可能会失败而忽视它们是愚蠢的。这就像你听说有些飞机坠毁了，有些船沉没了，你就决定只用步行和游泳的方式环游世界。敏捷不可能不建立在他人的工作之上。**

## **你应该停止使用和构建 AWS 吗？不要！**

**好的。最后，假设你问我这件事对我使用和构建 AWS 的意愿有多大影响——依赖它们。我回答你，“一点也不。”**

**这并不是说我喜欢这样的中断，也不是说我会忽略它们再次发生的可能性。但就像我仍然自信地乘飞机旅行，相信飞行员胜过相信我自己驾驶那些飞机一样，我仍然比我自己一个人更好地利用 AWS——他们提供的整套服务，包括缺点等等。我会非常准确地说你也是。**

**所以不要反应过度，也不要反应不足。**

**将你所学到的作为数据点与所有其他数据点结合起来。抛开过去的感觉和本能反应，做出理性的决定。当你意识到你所做的决定并不都是完美的，那么就运用这种无可指责的事后分析技巧，从这种情况中吸取教训，做得更好。我想，这就是我们每个人对自己的期望。**

**所以，照顾好你身边的人，拥抱#HugOps，继续做个牛逼的云大师吧！**

### **跟上 AWS 的所有事情**

**想要跟上 AWS 的所有事情？在推特[上关注 ACG](https://twitter.com/acloudguru)和[脸书](https://www.facebook.com/acloudguru)，[在 YouTube 上订阅云专家](https://www.youtube.com/c/AcloudGuru/?sub_confirmation=1)的每周 AWS 更新，并在 [Discord](http://discord.gg/acloudguru) 上加入对话。**

**想了解更多关于云和 AWS 的信息吗？查看我们每月更新的[免费课程](https://acloudguru.com/blog/news/whats-free-at-acg)的轮换阵容。(不需要信用卡！)**